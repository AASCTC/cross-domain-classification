{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/python/3.10.8/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: py7zr in /usr/local/python/3.10.8/lib/python3.10/site-packages (0.20.5)\n",
      "Requirement already satisfied: texttable in /usr/local/python/3.10.8/lib/python3.10/site-packages (from py7zr) (1.6.7)\n",
      "Requirement already satisfied: pycryptodomex>=3.6.6 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from py7zr) (3.18.0)\n",
      "Requirement already satisfied: pyzstd>=0.14.4 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from py7zr) (0.15.9)\n",
      "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from py7zr) (1.0.0)\n",
      "Requirement already satisfied: pybcj>=0.6.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from py7zr) (1.0.1)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from py7zr) (0.2.3)\n",
      "Requirement already satisfied: brotli>=1.0.9 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from py7zr) (1.0.9)\n",
      "Requirement already satisfied: inflate64>=0.3.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from py7zr) (0.3.1)\n",
      "Requirement already satisfied: psutil in /home/codespace/.local/lib/python3.10/site-packages (from py7zr) (5.9.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /usr/local/python/3.10.8/lib/python3.10/site-packages (4.9.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.8/lib/python3.10/site-packages (4.64.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install nltk\n",
    "%pip install scikit-learn\n",
    "%pip install beautifulsoup4\n",
    "%pip install py7zr\n",
    "%pip install lxml\n",
    "%pip install tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML To Structed XML\n",
    "\n",
    "This notebook will demonstrate a rudimentary process to convert raw HTML downloaded from the internet into a structured XML file tagged with keywords that describe it best, using Machine Learning.\n",
    "\n",
    "It only takes into account the HTML body, the title tag, and the alt-tags of media. In particular, *Javascript* cannot be executed, so pages which get their content from a server cannot be processed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "Our dataset consists of thousands of website homepages as input data, from a subset of the Majestic Million database, gathered in the span of two months, and over a million classifications obtained from Wikidata for tagging each element of the XML. Some websites could not be downloaded because of HTTP errors. Webpages that do not have a sufficient amount of tokens were removed.\n",
    "\n",
    "The classifications are stored as a JSON file with keys corresponding to the IDs of the types, in the 'wd:' or Wikidata namespace, and values being the titles of those IDs.\n",
    "\n",
    "The websites are compressed into a tarred XZ archive, which must be decompressed manually and placed into the `data/sites` folder.\n",
    "\n",
    "First we shall load the classification data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py7zr\n",
    "\n",
    "with py7zr.SevenZipFile(\"sites.7z\", mode='r') as z:\n",
    "    z.extractall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already extracted the data in a previous run, just skip to this block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic types database\n",
    "\n",
    "Next we will load the dataset containing the list of Wikidata types, in the wdata: namespace, and their corresponding titles. This will be used to map the identified keyword to a semantic type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('data/entities.json') as f:\n",
    "    entities = json.load(f)\n",
    "\n",
    "df = pd.DataFrame.from_dict(entities, orient='index', columns=['value']) #FIXME It seems that this is unused\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the input data\n",
    "\n",
    "We need to ensure that the input HTML pages have styles and Javascript as well as other kinds of behavior tags are removed, because they currently cannot be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from bs4 import MarkupResemblesLocatorWarning, XMLParsedAsHTMLWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup, Tag, Comment\n",
    "import re\n",
    "\n",
    "def convert_to_element(tag):\n",
    "    \"\"\"\n",
    "    Method used to convert Beautifulsoup HTML tags into XML elements.\n",
    "    \"\"\"\n",
    "    if isinstance(tag, Tag):\n",
    "        element = ET.Element(tag.name, tag.attrs)\n",
    "        for child in tag.contents:\n",
    "            if isinstance(child, Tag):\n",
    "                sub_element = convert_to_element(child)\n",
    "                element.append(sub_element)\n",
    "            else:\n",
    "                element.text = child\n",
    "        return element\n",
    "    else:\n",
    "        return ET.Element(tag)\n",
    "\n",
    "def detect_language(soup):\n",
    "    return soup.find('html') and soup.find('html').get('lang') or 'en-us'\n",
    "\n",
    "def extract_body(html):\n",
    "    \"\"\"\n",
    "    Extracts the body of the HTML, stripping out extraneous visual elemenets.\n",
    "    The keywords (names of the site) are also obtained.\n",
    "    \"\"\"\n",
    "    html = re.sub(\"\\s{2,}\", \" \", html)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "    # Do not process non-english sites\n",
    "    lang = detect_language(soup).lower()\n",
    "    if lang != \"en\" and not lang.startswith(\"en-\") and not lang.startswith(\"en_\"):\n",
    "        return None, set()\n",
    "\n",
    "    # Remove whitespace nodes\n",
    "    def remove_whitespace_nodes(node):\n",
    "        if not isinstance(node, str):\n",
    "            for child in node.contents:\n",
    "                if isinstance(child, str) and len(child.strip()) == 0:\n",
    "                    child.extract()\n",
    "                else:\n",
    "                    remove_whitespace_nodes(child)\n",
    "\n",
    "    remove_whitespace_nodes(soup)\n",
    "\n",
    "    # Remove script tags (JavaScript)\n",
    "    for script in soup.find_all('script'):\n",
    "        script.extract()\n",
    "    for script in soup.find_all('noscript'):\n",
    "        script.extract()\n",
    "\n",
    "    # Remove style tags (CSS)\n",
    "    for style in soup.find_all('style'):\n",
    "        style.extract()\n",
    "\n",
    "    # Remove comments\n",
    "    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        comment.extract()\n",
    "    \n",
    "    # Remove SVG images because these have complex\n",
    "    # drawing paths that we are not interested in\n",
    "    for style in soup.find_all('svg'):\n",
    "        style.extract()\n",
    "\n",
    "    # For the same reason, discard Form and Input elements.\n",
    "    for style in soup.find_all('form'):\n",
    "        style.extract()\n",
    "    for style in soup.find_all('input'):\n",
    "        style.extract()\n",
    "\n",
    "    # Extract body\n",
    "    body = copy(soup.body)\n",
    "    if not body:\n",
    "        return None, set()\n",
    "\n",
    "    # Remove \"class\" attributes from all elements\n",
    "    def remove_class_attributes(tag):\n",
    "        #if tag.has_attr('class'):\n",
    "        #    del tag['class']\n",
    "        \n",
    "        # Also remove data- attributes\n",
    "        # and aria-, style CSS, IDs, and other noisy\n",
    "        # attributes.\n",
    "        for attr in list(tag.attrs):\n",
    "            if not attr in [\"alt\", \"title\", \"src\", \"href\"]:\n",
    "                del tag[attr]\n",
    "\n",
    "            # Also remove empty keys\n",
    "            elif tag[attr].strip() == \"\":\n",
    "                del tag[attr]\n",
    "        \n",
    "        for child in tag.children:\n",
    "            if isinstance(child, Tag):\n",
    "                remove_class_attributes(child)\n",
    "\n",
    "    remove_class_attributes(body)\n",
    "\n",
    "    # Remove empty nodes\n",
    "    def remove_empty_nodes(tag):\n",
    "        children = tag.contents\n",
    "        for child in children:\n",
    "            if isinstance(child, Tag):\n",
    "                if len(child.contents) > 0:\n",
    "                    remove_empty_nodes(child)\n",
    "                if len(child.contents) == 0 and len(child.attrs.keys()) == 0:\n",
    "                    child.extract()\n",
    "\n",
    "    remove_empty_nodes(body)\n",
    "\n",
    "    def collapse_element(element):\n",
    "        if len(element.contents) == 1 and isinstance(element.contents[0], Tag):\n",
    "            child = element.contents[0]\n",
    "            element.replace_with(child)\n",
    "\n",
    "    # Recursively collapse elements\n",
    "    def process_element(element):\n",
    "        for child in element.children:\n",
    "            if isinstance(child, Tag):\n",
    "                process_element(child)\n",
    "                collapse_element(child)\n",
    "\n",
    "    process_element(body)\n",
    "    \n",
    "    def get_alphanumeric_tokens(input_string):\n",
    "        # Define the regular expression pattern to match alphanumeric tokens\n",
    "        pattern = r'\\b\\w+\\b'\n",
    "        \n",
    "        # Find all matches of the pattern in the input string\n",
    "        matches = re.findall(pattern, input_string)\n",
    "        \n",
    "        # Remove empty strings from the list of matches\n",
    "        alphanumeric_tokens = [token.lower() for token in matches if token.strip()]\n",
    "        \n",
    "        return alphanumeric_tokens\n",
    "    \n",
    "    # Add the title\n",
    "    if soup.title:\n",
    "        title = soup.new_tag('title')\n",
    "        title.string = soup.title.text\n",
    "        body.insert(0, title)\n",
    "    \n",
    "    # Take note of the site name because eventually they will be filtered out.\n",
    "    # Find the first occurrence of <meta property=\"og:site_name\" ...>\n",
    "    meta_tag = soup.find('meta', attrs={'property': 'og:site_name'})\n",
    "\n",
    "    keywords = set()\n",
    "    if meta_tag:\n",
    "        # Get the value of the \"content\" attribute\n",
    "        keyword = meta_tag.get('content')\n",
    "        if keyword:\n",
    "            keywords.update(get_alphanumeric_tokens(keyword))\n",
    "            \n",
    "            # See if the're a .com or similar at the end of the content\n",
    "            # and get rid of it, to use as a new keyword.\n",
    "            key = '.'.join(keyword.split('.')[:-1])\n",
    "            if key != keyword:\n",
    "                keywords.add(key)\n",
    "    \n",
    "    # Convert body contents to XML\n",
    "    xml = str(body)\n",
    "\n",
    "    return xml, keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to clean each of the webpages we have in the `data/sites` directory, one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "def list_html_files(directory_path):\n",
    "    html_files = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith('.html'):\n",
    "            html_files.append(filename)\n",
    "    return html_files\n",
    "\n",
    "html_files = list_html_files(\"data/sites\")\n",
    "\n",
    "def process_html(html_file):\n",
    "    with open(f\"data/sites/{html_file}\", errors='ignore') as f:\n",
    "        html = f.read()\n",
    "\n",
    "    try:\n",
    "        keywords = set()\n",
    "        keywords.add(html_file[:-5])\n",
    "        xml_output, keywords = extract_body(html)\n",
    "\n",
    "        if xml_output:\n",
    "            dom = etree.fromstring(xml_output, etree.HTMLParser())\n",
    "            return (etree.tostring(dom, encoding=\"unicode\", pretty_print=False), keywords)\n",
    "    except RecursionError:\n",
    "        # Crazy or maliciously crafted website that makes us crash, skip it\n",
    "        pass\n",
    "\n",
    "    return None, None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Tokenization\n",
    "\n",
    "To tokenize the XML content into words using the xml.dom.minidom module, you can extract the text content from the DOM nodes and then tokenize the text using appropriate techniques. \n",
    "\n",
    "## Punkt tokenizer\n",
    "\n",
    "The Punkt tokenizer is a pre-trained unsupervised machine learning tokenizer available in the NLTK (Natural Language Toolkit) library. It is designed specifically for tokenizing natural language text, capable of handling various punctuation patterns and ambiguous word boundaries. It uses a combination of unsupervised and supervised learning techniques to determine sentence boundaries and word tokenization, and has been trained on large corpora and can handle a wide range of languages and text genres.\n",
    "\n",
    "Punkt is a widely used and reliable tool for tokenizing natural language text in many NLP (Natural Language Processing) tasks, including text analysis, information retrieval, and machine learning algorithms that require tokenized input, but it must be downloaded first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "\n",
    "def extract_text_from_tags(dom):\n",
    "    # This method is a stub. It is defined in the next cell.\n",
    "    etree.fromstring(dom)\n",
    "\n",
    "class DOMElementInfo:\n",
    "    def __init__(self, element=None):\n",
    "        if isinstance(element, etree._Element):\n",
    "            self.element_name = element.tag\n",
    "            self.attributes = {attr: value for attr, value in element.attrib.items()}\n",
    "            self.user_data = etree.tostring(element, encoding=\"unicode\", pretty_print=False)\n",
    "            self.children = [DOMElementInfo(child) for child in element if child is not None]\n",
    "        else:\n",
    "            self.element_name = \"\"\n",
    "            self.attributes = {}\n",
    "            self.user_data = []\n",
    "            self.children = []\n",
    "\n",
    "    def to_xml(self):\n",
    "        element = etree.Element(self.element_name)\n",
    "        for attr, value in self.attributes.items():\n",
    "            element.set(attr, value)\n",
    "        # Write user data - assume list\n",
    "        if self.user_data:\n",
    "            element.set(\"templaterules-keywords\", ','.join(list(set(keyword for keyword in self.user_data if keyword))))\n",
    "        for child_info in self.children:\n",
    "            child_element = child_info.to_xml()\n",
    "            element.append(child_element)\n",
    "        return element\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_xml_declaration(input_string):\n",
    "        # Use regular expression to find and remove the <?xml ... ?> substring\n",
    "        pattern = r\"<\\?xml.*?\\>\"\n",
    "        return re.sub(pattern, '', input_string)\n",
    "        #input_string = re.sub(pattern, '', input_string)\n",
    "\n",
    "        # The input dataset chokes when this sequence is found.\n",
    "        #pattern = r\"<\\/?https:>\"\n",
    "        #return re.sub(pattern, '', input_string)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def try_extract(dom):\n",
    "        attempts = 0\n",
    "        while True:\n",
    "            attempts += 1\n",
    "            if attempts > 100:\n",
    "                # Give up\n",
    "                raise RuntimeError(\"This DOM is impossible to resolve\")\n",
    "            try:\n",
    "                extract_text_from_tags(dom)\n",
    "                return dom\n",
    "            except etree.XMLSyntaxError as e:\n",
    "                line, column = DOMElementInfo.extract_line_column_from_xml_error(str(e))\n",
    "                if not line or not column:\n",
    "                    raise e\n",
    "                dom = DOMElementInfo.remove_char_at_position(dom, line, column)\n",
    "                continue\n",
    "            \n",
    "    @staticmethod\n",
    "    def extract_line_column_from_xml_error(error_message):\n",
    "        # Regular expression pattern to find line and column numbers from the error message\n",
    "        pattern = r\"line (\\d+), column (\\d+)\"\n",
    "        match = re.search(pattern, error_message)\n",
    "        \n",
    "        if match:\n",
    "            line_number = int(match.group(1))\n",
    "            column_number = int(match.group(2))\n",
    "            return line_number, column_number\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_char_at_position(text, line_number, column_number):\n",
    "        lines = text.split('\\n')\n",
    "\n",
    "        if line_number <= 0 or line_number > len(lines):\n",
    "            return text\n",
    "\n",
    "        line_index = line_number - 1\n",
    "        line = lines[line_index]\n",
    "\n",
    "        if column_number <= 0 or column_number > len(line):\n",
    "            return text\n",
    "\n",
    "        # Calculate the position of the character to remove in the original string\n",
    "        char_index_to_remove = sum(len(lines[i]) + 1 for i in range(line_index)) + column_number - 1\n",
    "\n",
    "        # Create a new string without the character at the specified position\n",
    "        new_text = text[:char_index_to_remove] + text[char_index_to_remove + 1:]\n",
    "\n",
    "        return new_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from lxml import etree\n",
    "\n",
    "def extract_text_from_tags(dom):\n",
    "    dom = etree.fromstring(dom)\n",
    "    body_text = ''\n",
    "    h1_text = ''\n",
    "    h2_text = ''\n",
    "    h3_text = ''\n",
    "    h4_text = ''\n",
    "    h5_text = ''\n",
    "    h6_text = ''\n",
    "    other_text = ''\n",
    "\n",
    "    def extract_text(node, header_level=-1):\n",
    "        nonlocal body_text, h1_text, h2_text, h3_text, h4_text, h5_text, h6_text, other_text\n",
    "\n",
    "        if node.tag == etree.Comment:\n",
    "            return\n",
    "\n",
    "        if isinstance(node, etree._ElementUnicodeResult):\n",
    "            node_data = node\n",
    "        else:\n",
    "            node_data = node.text\n",
    "\n",
    "        if node_data and isinstance(node_data, str):\n",
    "            text = node_data.strip()\n",
    "            if header_level == 0:\n",
    "                body_text += ' ' + text\n",
    "            elif header_level == 1:\n",
    "                h1_text += ' ' + text\n",
    "            elif header_level == 2:\n",
    "                h2_text += ' ' + text\n",
    "            elif header_level == 3:\n",
    "                h3_text += ' ' + text\n",
    "            elif header_level == 4:\n",
    "                h4_text += ' ' + text\n",
    "            elif header_level == 5:\n",
    "                h5_text += ' ' + text\n",
    "            elif header_level == 6:\n",
    "                h6_text += ' ' + text\n",
    "            else:\n",
    "                other_text += ' ' + text\n",
    "\n",
    "        for child in node:\n",
    "            extract_text(child, header_level + 1)\n",
    "\n",
    "    body = dom\n",
    "    for child in body:\n",
    "        tag_name = child.tag.lower()\n",
    "        if tag_name == 'title':\n",
    "            extract_text(child, 0)\n",
    "        elif tag_name == 'h1':\n",
    "            extract_text(child, 1)\n",
    "        elif tag_name == 'h2':\n",
    "            extract_text(child, 2)\n",
    "        elif tag_name == 'h3':\n",
    "            extract_text(child, 3)\n",
    "        elif tag_name == 'h4':\n",
    "            extract_text(child, 4)\n",
    "        elif tag_name == 'h5':\n",
    "            extract_text(child, 5)\n",
    "        elif tag_name == 'h6':\n",
    "            extract_text(child, 6)\n",
    "        else:\n",
    "            extract_text(child)\n",
    "\n",
    "    return ' '.join([body_text.strip(), h1_text.strip(), h2_text.strip(), h3_text.strip(), h4_text.strip(),\n",
    "                     h5_text.strip(), h6_text.strip(), other_text.strip()])\n",
    "\n",
    "\n",
    "# Define common stopwords/conjunctions/pronouns\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def tokenize_dom(dom, keywords):\n",
    "    try:\n",
    "        dom = extract_text_from_tags(dom)\n",
    "    except etree.XMLSyntaxError:\n",
    "        return []\n",
    "    except AttributeError:\n",
    "        # Because cython-function-or-method in element.tag nonsense\n",
    "        return []\n",
    "    \n",
    "    tokenized_text = nltk.word_tokenize(dom)\n",
    "    \n",
    "    # Remove tokens with symbols using regular expressions\n",
    "    tokenized_text = [token for token in tokenized_text if re.match(r'^[-a-zA-Z0-9]+$', token)]\n",
    "\n",
    "    # Remove tokens that are entirely numbers\n",
    "    tokenized_text = [token for token in tokenized_text if not token.isdigit()]\n",
    "\n",
    "    # Remove tokens that are one character long\n",
    "    tokenized_text = [token for token in tokenized_text if len(token) > 1]\n",
    "\n",
    "    # Remove common stopwords\n",
    "    tokenized_text = [token for token in tokenized_text if token.lower() not in stopwords_list]\n",
    "\n",
    "    # Convert titlecase tokens to lowercase, except for ALL CAPS tokens\n",
    "    tokenized_text = [token.lower() if not token.isupper() else token for token in tokenized_text]\n",
    "\n",
    "    # And finally remove the website name(s) from the list of tokens\n",
    "    tokenized_text = [token for token in tokenized_text if token not in keywords]\n",
    "    \n",
    "    return tokenized_text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token weights\n",
    "\n",
    "### This has been removed from the calculation because it kept producing repetitive n-grams as the most common tokens.\n",
    "\n",
    "Next we are going to prepare the vectorizers by inserting more tokens based on how close to the beginning of the list the token is. Because of the way the list is ordered, the title will be more emphasized than the headings, which will be successively more emphasized than the paragraphs and other elements.\n",
    "\n",
    "We will use an exponential distriution with $\\lambda = t_I$ with $t_I$ representing the number of tokens at the beginning of the list that are interesting (and to be boosted). $t_I$ will be hardcoded to $min(\\frac{t}{10}, 25)$ in all training data, where $t$ is the total number of tokens. This is to accomodate small webpages with small token lists close to $25$.\n",
    "\n",
    "Consequentially we are going to use as our horizontal scaling (expanding specifically) factor $\\frac{1}{\\lambda^{2}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def weight_tokens(tokenized_text):\n",
    "    tokens = len(tokenized_text)\n",
    "    interesting_tokens = min(tokens/10, 25)\n",
    "    lam = interesting_tokens\n",
    "    tokenized_text_lists=[]\n",
    "    for i in range(0, len(tokenized_text)):\n",
    "        x = i+1 # Make it one-based\n",
    "        \n",
    "        # Fractional additions are not possible so just truncate the addition number to an integer,\n",
    "        # discarding the decimal place.\n",
    "        y = int(lam * np.exp(-lam * 1/(interesting_tokens**2)*x) + 1)\n",
    "        tokenized_text_lists.append([tokenized_text[i]]*y)\n",
    "    tokenized_text_2 = flatten(tokenized_text_lists)\n",
    "    tokenized_text = tokenized_text_2\n",
    "    del tokenized_text_2\n",
    "    return tokenized_text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token vectorizers\n",
    "\n",
    "At this point, we have a choice between two machine learning algorithms for classifying the data - we can use Bag of Words which just counts the frequency of the terms in the documet, or we can use TF/IDF which stands for *Term Frequency times the Inverse Document Frequency* (the \"IDF\" being the *inverse* of the number of times the term appears in the document).\n",
    "\n",
    "TF/IDF is more readily compared between different documents and so this is the metric that we will settle with. We will consider only terms with a frequency of 0.1 or greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def train_tokens_model(weighted_text):\n",
    "    threshold = 0.1\n",
    "    # Calculate TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "    tfidf = tfidf_vectorizer.fit_transform([\" \".join(weighted_text)])\n",
    "\n",
    "    # Get top tokens according to the threshold and TF-IDF scores\n",
    "    feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "    input_data = [x for x in zip(feature_names_tfidf, tfidf.toarray()[0]) if x[1] >= threshold]\n",
    "    top_tokens_tfidf = sorted(input_data, key=lambda x: x[1], reverse=True)[:3]\n",
    "    return top_tokens_tfidf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic type inference\n",
    "\n",
    "Using the semantic type Wikidata we have imported, now we can turn each keyword into a symantic identifier representing a structured type as defined in [XML Schema](https://www.w3.org/XML/Schema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def collect_non_zero_indices(vector1, vector2):\n",
    "    # Get the indices of non-zero elements in vector1\n",
    "    non_zero_indices = np.nonzero(vector1)[0]\n",
    "\n",
    "    # Create two new vectors with non-zero indices\n",
    "    new_vector1 = vector1[non_zero_indices]\n",
    "    new_vector2 = vector2[non_zero_indices]\n",
    "\n",
    "    return non_zero_indices, new_vector1, new_vector2\n",
    "\n",
    "def find_most_similar_identifier_preprocess(dictionary):\n",
    "    # Create a list of phrases from the dictionary values\n",
    "    phrases = [phrase.lower() for phrase in list(dictionary.values())]\n",
    "\n",
    "    # Create a CountVectorizer and fit-transform the text data\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectors = vectorizer.fit_transform(phrases)\n",
    "    \n",
    "    norm_vectors = csr_matrix(sparse.linalg.norm(vectors, axis=1)).transpose()\n",
    "\n",
    "    return vectors, vectorizer, norm_vectors\n",
    "\n",
    "def find_most_similar_identifier(input_word, vectors, vectorizer, norm_vectors, dictionary):\n",
    "    # Transform the input word using the pre-fitted vectorizer\n",
    "    input_vector = vectorizer.transform([input_word])\n",
    "\n",
    "    dot = csr_matrix.dot(vectors, input_vector.T)\n",
    "\n",
    "    if not dot.nnz:\n",
    "        return \"\" # Nothing in the dictionary is similar to it.\n",
    "    \n",
    "    non_zero_indicies, dot, norm_vectors = collect_non_zero_indices(dot, norm_vectors)\n",
    "\n",
    "    norm_input = csr_matrix(sparse.linalg.norm(input_vector, axis=1)).transpose()\n",
    "\n",
    "    # Calculate cosine similarity scores between input_vector and phrase vectors\n",
    "    similarity_scores = csr_matrix(dot / (norm_vectors * norm_input))\n",
    "    \n",
    "    # Find the index of the most similar phrase\n",
    "    most_similar_index = non_zero_indicies[csr_matrix.argmax(similarity_scores)]\n",
    "\n",
    "    # Get the corresponding identifier from the dictionary\n",
    "    most_similar_identifier = list(dictionary.keys())[most_similar_index]\n",
    "\n",
    "    # In case the dictionary give us a non-identifier\n",
    "    if not re.match(r'^Q\\d+$', most_similar_identifier):\n",
    "        return ''\n",
    "\n",
    "    return most_similar_identifier\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalability\n",
    "\n",
    "If we run each processing stage one of the time with all of the input elements at once, it is highly likely the kernel will fail before that particular stage is complete. During tests, we found that the dataset of around 15,000 web pages would consume about 8 gigabytes of memory when fully tokenized. Production datasets will almost certainly be an order of magnitude larger than this figure, so it requires an alternate approach to processing.\n",
    "\n",
    "Instead of running each processing stage one at a time, we can split the input data into batches of reasonable sizes and run the entire pipeline on the batches one at a time, beginning and ending with disk I/O access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_with_progress(html_file):\n",
    "    try:\n",
    "        result = process_html(html_file)\n",
    "    except RuntimeError:\n",
    "        result = None\n",
    "    except ValueError:\n",
    "        result = None\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_element_info(element):\n",
    "    element = DOMElementInfo.remove_xml_declaration(element)\n",
    "    element = DOMElementInfo.try_extract(element)\n",
    "    info = DOMElementInfo(etree.fromstring(element))\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "def tokenize_dom_with_progress(dom_info, keywords):\n",
    "    def process_element_with_progress(element_info):\n",
    "        try:\n",
    "            element_info = copy(element_info)\n",
    "            if (element_info.user_data):\n",
    "                element_info.user_data = tokenize_dom(element_info.user_data, keywords)\n",
    "        except RuntimeError:\n",
    "            element_info.user_data = []\n",
    "            \n",
    "        for i in range(len(element_info.children)):\n",
    "            child = element_info.children[i]\n",
    "            element_info.children[i] = process_element_with_progress(child)\n",
    "        \n",
    "        return element_info\n",
    "\n",
    "    # Process DOM elements recursively\n",
    "    processed_results = process_element_with_progress(dom_info)\n",
    "\n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "def weight_tokens_with_progress(tokenized_text):\n",
    "    def weight_tokens_with_progress_recursive(tokenized_text):\n",
    "        tokenized_text = copy(tokenized_text)\n",
    "        tokenized_text.user_data = weight_tokens(tokenized_text.user_data)\n",
    "        \n",
    "        for i in range(len(tokenized_text.children)):\n",
    "            child = tokenized_text.children[i]\n",
    "            tokenized_text.children[i] = weight_tokens_with_progress_recursive(child)\n",
    "        \n",
    "        return tokenized_text\n",
    "\n",
    "    # Process elements recursively\n",
    "    processed_results = weight_tokens_with_progress_recursive(tokenized_text)\n",
    "\n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "def train_tokens_model_with_progress(weighted_text):\n",
    "    def train_tokens_model_with_progress_recursive(weighted_text):\n",
    "        try:\n",
    "            weighted_text = copy(weighted_text)\n",
    "            weighted_text.user_data = train_tokens_model(weighted_text.user_data)\n",
    "        except ValueError:\n",
    "            weighted_text.user_data = []\n",
    "        \n",
    "        for i in range(len(weighted_text.children)):\n",
    "            child = weighted_text.children[i]\n",
    "            weighted_text.children[i] = train_tokens_model_with_progress_recursive(child)\n",
    "        \n",
    "        return weighted_text\n",
    "\n",
    "    # Process elements recursively\n",
    "    processed_results = train_tokens_model_with_progress_recursive(weighted_text)\n",
    "\n",
    "    return processed_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "def find_most_similar_identifier_with_progress(top_tokens_tfidf_data, vectors, vectorizer, norm_vectors, entities):\n",
    "    def find_most_similar_identifier_with_progress_recursive(top_tokens_tfidf_data, vectors, vectorizer, norm_vectors, entities):\n",
    "        try:\n",
    "            top_tokens_tfidf_data = copy(top_tokens_tfidf_data)\n",
    "            top_tokens_tfidf_data.user_data = [find_most_similar_identifier(input_word[0], vectors, vectorizer, norm_vectors, entities)\n",
    "                                               for input_word in top_tokens_tfidf_data.user_data]\n",
    "        except ValueError:\n",
    "            top_tokens_tfidf_data.user_data = []\n",
    "        \n",
    "        for i in range(len(top_tokens_tfidf_data.children)):\n",
    "            child = top_tokens_tfidf_data.children[i]\n",
    "            top_tokens_tfidf_data.children[i] = find_most_similar_identifier_with_progress_recursive(child, vectors, vectorizer, norm_vectors, entities)\n",
    "        \n",
    "        return top_tokens_tfidf_data\n",
    "\n",
    "    # Process elements recursively\n",
    "    processed_results = find_most_similar_identifier_with_progress_recursive(top_tokens_tfidf_data, vectors, vectorizer, norm_vectors, entities)\n",
    "\n",
    "    return processed_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.000% |██████████| 5/5 [01:20<00:00, 16.13s/it]\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "progress_format = \"{percentage:.3f}% |{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\"\n",
    "\n",
    "\n",
    "def pipeline_with_progress(html_file):\n",
    "    site_html, keywords = process_html_with_progress(html_file)\n",
    "    if not site_html:\n",
    "        progress_bar.update(1)\n",
    "        return None\n",
    "    try:\n",
    "        dom_info = gather_element_info(site_html)\n",
    "    except RuntimeError:\n",
    "        progress_bar.update(1)\n",
    "        return None\n",
    "    if not dom_info:\n",
    "        progress_bar.update(1)\n",
    "        return None\n",
    "    \n",
    "    tokenized_text = tokenize_dom_with_progress(dom_info, keywords)\n",
    "    if not tokenized_text:\n",
    "        progress_bar.update(1)\n",
    "        return None\n",
    "    #weighted_text = weight_tokens_with_progress(tokenized_text)\n",
    "    top_tokens_tfidf_data = train_tokens_model_with_progress(tokenized_text)\n",
    "    if not top_tokens_tfidf_data:\n",
    "        progress_bar.update(1)\n",
    "        return None\n",
    "    most_similar_identifiers = find_most_similar_identifier_with_progress(top_tokens_tfidf_data, vectors, vectorizer, norm_vectors, entities)\n",
    "    if not most_similar_identifiers:\n",
    "        progress_bar.update(1)\n",
    "        return None\n",
    "    try:\n",
    "        result = most_similar_identifiers.to_xml()\n",
    "        xml_file = re.sub('\\.html', '.xml', html_file)\n",
    "        with open(f\"outputs/{xml_file}\", \"w\") as f:\n",
    "            f.write(etree.tostring(result, encoding=\"unicode\", pretty_print=False))\n",
    "    except TypeError as e:\n",
    "        print(\"Hey does this look familiar?\")\n",
    "        result = None\n",
    "    progress_bar.update(1)\n",
    "    return result\n",
    "\n",
    "def divide_into_batches(lst, batch_size):\n",
    "    return [lst[i:i+batch_size] for i in range(0, len(lst), batch_size)]\n",
    "\n",
    "# Feel free to change this depending on your memory size.\n",
    "batch_size = 64\n",
    "\n",
    "# Divide the work into batches to be processed at once.\n",
    "# Maximum memory usage is the amount the largest dataset in the batch uses.\n",
    "data_batches = divide_into_batches(html_files[0:5], batch_size)\n",
    "\n",
    "# Use tqdm to create a progress bar for the loop\n",
    "with tqdm(total=len(html_files[0:5]), bar_format=progress_format) as progress_bar:\n",
    "    vectors, vectorizer, norm_vectors = find_most_similar_identifier_preprocess(entities)\n",
    "    for batch in data_batches:\n",
    "        # Create a ThreadPoolExecutor with the number of cores available\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_cores) as executor:\n",
    "            # Use the map function to distribute the processing across multiple threads\n",
    "            results = list(executor.map(pipeline_with_progress, batch))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
