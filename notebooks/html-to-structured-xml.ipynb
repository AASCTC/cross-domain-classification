{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/python/3.10.8/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting py7zr\n",
      "  Downloading py7zr-0.20.5-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting texttable (from py7zr)\n",
      "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
      "Collecting pycryptodomex>=3.6.6 (from py7zr)\n",
      "  Downloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyzstd>=0.14.4 (from py7zr)\n",
      "  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyppmd<1.1.0,>=0.18.1 (from py7zr)\n",
      "  Downloading pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pybcj>=0.6.0 (from py7zr)\n",
      "  Downloading pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr)\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Collecting brotli>=1.0.9 (from py7zr)\n",
      "  Downloading Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting inflate64>=0.3.1 (from py7zr)\n",
      "  Downloading inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /home/codespace/.local/lib/python3.10/site-packages (from py7zr) (5.9.5)\n",
      "Installing collected packages: texttable, brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, py7zr\n",
      "Successfully installed brotli-1.0.9 inflate64-0.3.1 multivolumefile-0.2.3 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.9 texttable-1.6.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install py7zr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML To Unstructed XML\n",
    "\n",
    "This notebook will demonstrate a rudimentary process to convert raw HTML downloaded from the internet into a structured XML file tagged with keywords that describe it best, using Machine Learning.\n",
    "\n",
    "It only takes into account the HTML body, the title tag, and the alt-tags of media. In particular, *Javascript* cannot be executed, so pages which get their content from a server cannot be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "Our dataset consists of tens of thousands of website homepages as input data, from a subset of the Majestic Million database, gathered in the span of two months, and over a million classifications obtained from Wikidata for tagging each element of the XML. Some websites could not be downloaded because of HTTP errors. Webpages that do not have a sufficient amount of tokens were removed.\n",
    "\n",
    "The classifications are stored as a JSON file with keys corresponding to the IDs of the types, in the 'wd:' or Wikidata namespace, and values being the titles of those IDs.\n",
    "\n",
    "The websites are compressed into a tarred XZ archive, which must be decompressed manually and placed into the `data/sites` folder.\n",
    "\n",
    "First we shall load the classification data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sites.7z'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpy7zr\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mwith\u001b[39;00m py7zr\u001b[39m.\u001b[39;49mSevenZipFile(\u001b[39m\"\u001b[39;49m\u001b[39msites.7z\u001b[39;49m\u001b[39m\"\u001b[39;49m, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m z:\n\u001b[1;32m      4\u001b[0m     z\u001b[39m.\u001b[39mextractall()\n\u001b[1;32m      6\u001b[0m os\u001b[39m.\u001b[39mchdir(\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/py7zr/py7zr.py:344\u001b[0m, in \u001b[0;36mSevenZipFile.__init__\u001b[0;34m(self, file, mode, filters, dereference, password, header_encryption, blocksize, mp)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m file\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 344\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(file, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    345\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    346\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(file, \u001b[39m\"\u001b[39m\u001b[39mw+b\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sites.7z'"
     ]
    }
   ],
   "source": [
    "import py7zr\n",
    "\n",
    "with py7zr.SevenZipFile(\"sites.7z\", mode='r') as z:\n",
    "    z.extractall()\n",
    "    \n",
    "os.chdir('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('data/entities.json') as f:\n",
    "    entities = json.load(f)\n",
    "\n",
    "df = pd.DataFrame.from_dict(entities, orient='index', columns=['value'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the input data\n",
    "\n",
    "We need to ensure that the input HTML pages have styles and Javascript as well as other kinds of behavior tags are removed, because they currently cannot be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup, Tag, Comment\n",
    "import re\n",
    "\n",
    "def convert_to_element(tag):\n",
    "    \"\"\"\n",
    "    Method used to convert Beautifulsoup HTML tags into XML elements.\n",
    "    \"\"\"\n",
    "    if isinstance(tag, Tag):\n",
    "        element = ET.Element(tag.name, tag.attrs)\n",
    "        for child in tag.contents:\n",
    "            if isinstance(child, Tag):\n",
    "                sub_element = convert_to_element(child)\n",
    "                element.append(sub_element)\n",
    "            else:\n",
    "                element.text = child\n",
    "        return element\n",
    "    else:\n",
    "        return ET.Element(tag)\n",
    "\n",
    "def extract_body(html):\n",
    "    \"\"\"\n",
    "    Extracts the body of the HTML, stripping out extraneous visual elemenets.\n",
    "    The keywords (names of the site) are also obtained.\n",
    "    \"\"\"\n",
    "    html = re.sub(\"\\s{2,}\", \"\", html)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Remove whitespace nodes\n",
    "    def remove_whitespace_nodes(node):\n",
    "        if not isinstance(node, str):\n",
    "            for child in node.contents:\n",
    "                if isinstance(child, str) and len(child.strip()) == 0:\n",
    "                    child.extract()\n",
    "                else:\n",
    "                    remove_whitespace_nodes(child)\n",
    "\n",
    "    remove_whitespace_nodes(soup)\n",
    "\n",
    "    # Remove script tags (JavaScript)\n",
    "    for script in soup.find_all('script'):\n",
    "        script.extract()\n",
    "    for script in soup.find_all('noscript'):\n",
    "        script.extract()\n",
    "\n",
    "    # Remove style tags (CSS)\n",
    "    for style in soup.find_all('style'):\n",
    "        style.extract()\n",
    "\n",
    "    # Remove comments\n",
    "    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        comment.extract()\n",
    "    \n",
    "    # Remove SVG images because these have complex\n",
    "    # drawing paths that we are not interested in\n",
    "    for style in soup.find_all('svg'):\n",
    "        style.extract()\n",
    "\n",
    "    # For the same reason, discard Form and Input elements.\n",
    "    for style in soup.find_all('form'):\n",
    "        style.extract()\n",
    "    for style in soup.find_all('input'):\n",
    "        style.extract()\n",
    "\n",
    "    # Extract body\n",
    "    body = copy(soup.body)\n",
    "\n",
    "    # Remove \"class\" attributes from all elements\n",
    "    def remove_class_attributes(tag):\n",
    "        #if tag.has_attr('class'):\n",
    "        #    del tag['class']\n",
    "        \n",
    "        # Also remove data- attributes\n",
    "        # and aria-, style CSS, IDs, and other noisy\n",
    "        # attributes.\n",
    "        for attr in list(tag.attrs):\n",
    "            if not attr in [\"alt\", \"title\", \"src\", \"href\"]:\n",
    "                del tag[attr]\n",
    "\n",
    "            # Also remove empty keys\n",
    "            elif tag[attr].strip() == \"\":\n",
    "                del tag[attr]\n",
    "        \n",
    "        for child in tag.children:\n",
    "            if isinstance(child, Tag):\n",
    "                remove_class_attributes(child)\n",
    "\n",
    "    remove_class_attributes(body)\n",
    "\n",
    "    # Remove empty nodes\n",
    "    def remove_empty_nodes(tag):\n",
    "        children = tag.contents\n",
    "        for child in children:\n",
    "            if isinstance(child, Tag):\n",
    "                if len(child.contents) > 0:\n",
    "                    remove_empty_nodes(child)\n",
    "                if len(child.contents) == 0 and len(child.attrs.keys()) == 0:\n",
    "                    child.extract()\n",
    "\n",
    "    remove_empty_nodes(body)\n",
    "\n",
    "    def collapse_element(element):\n",
    "        if len(element.contents) == 1 and isinstance(element.contents[0], Tag):\n",
    "            child = element.contents[0]\n",
    "            element.replace_with(child)\n",
    "\n",
    "    # Recursively collapse elements\n",
    "    def process_element(element):\n",
    "        for child in element.children:\n",
    "            if isinstance(child, Tag):\n",
    "                process_element(child)\n",
    "                collapse_element(child)\n",
    "\n",
    "    process_element(body)\n",
    "    \n",
    "    # Add the title\n",
    "    title = soup.new_tag('title')\n",
    "    title.string = soup.title.text\n",
    "    body.insert(0, title)\n",
    "    \n",
    "    # Take note of the site name because eventually they will be filtered out.\n",
    "    # Find the first occurrence of <meta property=\"og:site_name\" ...>\n",
    "    meta_tag = soup.find('meta', attrs={'property': 'og:site_name'})\n",
    "\n",
    "    keywords = set()\n",
    "    if meta_tag:\n",
    "        # Get the value of the \"content\" attribute\n",
    "        keyword = meta_tag.get('content')\n",
    "        keywords.add(keyword)\n",
    "        \n",
    "        # See if the're a .com or similar at the end of the content\n",
    "        # and get rid of it, to use as a new keyword.\n",
    "        key = '.'.join(keyword.split('.')[:-1])\n",
    "        if key != keyword:\n",
    "            keywords.add(key)\n",
    "    \n",
    "    # Convert body contents to XML\n",
    "    xml = str(body)\n",
    "\n",
    "    return xml, keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to clean each of the webpages we have in the `data/sites` directory, one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
