{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e4714-10bc-4a3f-be78-f57db2702c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies first, then restart the kernel.\n",
    "%pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a91c74-e64b-4397-81cd-c0d2f61fb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup, Tag, Comment\n",
    "import xml.dom.minidom\n",
    "import re\n",
    "\n",
    "def convert_to_element(tag):\n",
    "    if isinstance(tag, Tag):\n",
    "        element = ET.Element(tag.name, tag.attrs)\n",
    "        for child in tag.contents:\n",
    "            if isinstance(child, Tag):\n",
    "                sub_element = convert_to_element(child)\n",
    "                element.append(sub_element)\n",
    "            else:\n",
    "                element.text = child\n",
    "        return element\n",
    "    else:\n",
    "        return ET.Element(tag)\n",
    "\n",
    "def extract_body(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Remove whitespace nodes\n",
    "    def remove_whitespace_nodes(node):\n",
    "        if not isinstance(node, str):\n",
    "            for child in node.contents:\n",
    "                if isinstance(child, str) and len(child.strip()) == 0:\n",
    "                    child.extract()\n",
    "                else:\n",
    "                    remove_whitespace_nodes(child)\n",
    "\n",
    "    remove_whitespace_nodes(soup)\n",
    "\n",
    "    # Remove script tags (JavaScript)\n",
    "    for script in soup.find_all('script'):\n",
    "        script.extract()\n",
    "    for script in soup.find_all('noscript'):\n",
    "        script.extract()\n",
    "\n",
    "    # Remove style tags (CSS)\n",
    "    for style in soup.find_all('style'):\n",
    "        style.extract()\n",
    "\n",
    "    # Remove comments\n",
    "    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        comment.extract()\n",
    "    \n",
    "    # Remove SVG images because these have complex\n",
    "    # drawing paths that we are not interested in\n",
    "    for style in soup.find_all('svg'):\n",
    "        style.extract()\n",
    "\n",
    "    # For the same reason, discard Form and Input elements.\n",
    "    for style in soup.find_all('form'):\n",
    "        style.extract()\n",
    "    for style in soup.find_all('input'):\n",
    "        style.extract()\n",
    "\n",
    "    # Extract body\n",
    "    body = soup.body\n",
    "\n",
    "    # Remove \"class\" attributes from all elements\n",
    "    def remove_class_attributes(tag):\n",
    "        #if tag.has_attr('class'):\n",
    "        #    del tag['class']\n",
    "        \n",
    "        # Also remove data- attributes\n",
    "        # and aria-, style CSS, IDs, and other noisy\n",
    "        # attributes.\n",
    "        for attr in list(tag.attrs):\n",
    "            if not attr in [\"alt\", \"title\", \"src\", \"href\"]:\n",
    "                del tag[attr]\n",
    "\n",
    "            # Also remove empty keys\n",
    "            elif tag[attr].strip() == \"\":\n",
    "                del tag[attr]\n",
    "            \n",
    "        #    if attr.startswith('data-'):\n",
    "        #        del tag[attr]\n",
    "        #    elif attr.startswith('aria-'):\n",
    "        #        del tag[attr]\n",
    "        #    elif attr == 'style':\n",
    "        #        del tag[attr]\n",
    "        #    elif attr == 'id':\n",
    "        #        del tag[attr]\n",
    "        #    elif attr in ['height', 'loading', 'width', 'tabindex', 'dir', 'target', 'role', 'type']:\n",
    "        #        del tag[attr]\n",
    "        \n",
    "        for child in tag.children:\n",
    "            if isinstance(child, Tag):\n",
    "                remove_class_attributes(child)\n",
    "\n",
    "    remove_class_attributes(body)\n",
    "\n",
    "    # Remove empty nodes\n",
    "    def remove_empty_nodes(tag):\n",
    "        children = tag.contents\n",
    "        for child in children:\n",
    "            if isinstance(child, Tag):\n",
    "                if len(child.contents) > 0:\n",
    "                    remove_empty_nodes(child)\n",
    "                if len(child.contents) == 0 and len(child.attrs.keys()) == 0:\n",
    "                    child.extract()\n",
    "\n",
    "    remove_empty_nodes(body)\n",
    "    \n",
    "    # Convert body contents to XML\n",
    "    xml = str(body)\n",
    "\n",
    "    return xml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fe0efe-1013-44ac-a4e1-5c3c69bfcb8d",
   "metadata": {},
   "source": [
    "## Example usage of the extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb197c-d6d6-42de-9920-e3ae9efa1303",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "<head>\n",
    "    <title>Example HTML</title>\n",
    "    <style>\n",
    "        body {\n",
    "            background-color: #f0f0f0;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1 class=\"heading\">Hello, world!</h1>\n",
    "    <p>This is an example HTML.</p>\n",
    "    <div class=\"container\" data-info=\"some-info\" data-value=\"42\">\n",
    "        <p class=\"info\">Some information</p>\n",
    "        <p class=\"note\">A note</p>\n",
    "    </div><div><div><div></div></div></div>\n",
    "    <script>\n",
    "        alert('This is JavaScript');\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "xml_output = re.sub('>\\n+<', '><', extract_body(html))\n",
    "\n",
    "\n",
    "dom = xml.dom.minidom.parseString(xml_output)\n",
    "formatted_xml = dom.toprettyxml(indent='  ')\n",
    "\n",
    "print(formatted_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defaa5a2-cb5a-4936-be57-3c2182644dd5",
   "metadata": {},
   "source": [
    "# Sample Yahoo Site Data starts here.\n",
    "\n",
    "We are going to use this as a starting point to process all the other HTML data that we downloaded as part of the dataset, into a \"clean\" representation which can then be converted into a tokenized representation for Machine Learning, or it can be coupled with other cross-domain data using a Template Rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28223de-6a4f-430e-81b5-016d58023611",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"../data/yahoo.com.html\") as f:\n",
    "    yahoo_html = f.read()\n",
    "\n",
    "xml_output = extract_body(yahoo_html)\n",
    "#print(xml_output[335671-100:335671+100])\n",
    "\n",
    "dom = xml.dom.minidom.parseString(xml_output)\n",
    "formatted_xml = dom.toprettyxml(indent='  ')\n",
    "\n",
    "print(formatted_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb33c5-9bab-4449-a74f-b12165bbf0da",
   "metadata": {},
   "source": [
    "# Data Tokenization\n",
    "\n",
    "To tokenize the XML content into words using the xml.dom.minidom module, you can extract the text content from the DOM nodes and then tokenize the text using appropriate techniques. \n",
    "\n",
    "## Punkt tokenizer\n",
    "\n",
    "The Punkt tokenizer is a pre-trained unsupervised machine learning tokenizer available in the NLTK (Natural Language Toolkit) library. It is designed specifically for tokenizing natural language text, capable of handling various punctuation patterns and ambiguous word boundaries. It uses a combination of unsupervised and supervised learning techniques to determine sentence boundaries and word tokenization, and has been trained on large corpora and can handle a wide range of languages and text genres.\n",
    "\n",
    "Punkt is a widely used and reliable tool for tokenizing natural language text in many NLP (Natural Language Processing) tasks, including text analysis, information retrieval, and machine learning algorithms that require tokenized input, but it must be downloaded first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f2ffc81-e2cb-42d7-a6f3-a46ae29e8383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcaed1b7-da92-4e92-ba0d-3d6c610ce0d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m tokenized_text \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenized_text \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Define common stopwords/conjunctions/pronouns\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m stopwords_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Remove common stopwords\u001b[39;00m\n\u001b[1;32m     42\u001b[0m tokenized_text \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenized_text \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords_list]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "# Code for tokenizing the XML\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def extract_text_from_tags(dom):\n",
    "    extracted_text = ''\n",
    "\n",
    "    def extract_text(node):\n",
    "        nonlocal extracted_text\n",
    "\n",
    "        if node.nodeType == node.TEXT_NODE:\n",
    "            extracted_text += ' ' + node.data.strip()\n",
    "        elif node.nodeType == node.ELEMENT_NODE:\n",
    "            if node.tagName == 'img':\n",
    "                if node.hasAttribute('alt'):\n",
    "                    extracted_text += ' ' + node.getAttribute('alt')\n",
    "            if node.hasAttribute('title'):\n",
    "                extracted_text += ' ' + node.getAttribute('title')\n",
    "\n",
    "            for child in node.childNodes:\n",
    "                extract_text(child)\n",
    "\n",
    "    body = dom.documentElement\n",
    "    for child in body.childNodes:\n",
    "        extract_text(child)\n",
    "\n",
    "    return extracted_text.strip()\n",
    "\n",
    "tokenized_text = nltk.word_tokenize(extract_text_from_tags(dom))\n",
    "# Remove tokens with symbols using regular expressions\n",
    "tokenized_text = [token for token in tokenized_text if re.match(r'^[-a-zA-Z0-9]+$', token)]\n",
    "\n",
    "# Remove tokens that are entirely numbers\n",
    "tokenized_text = [token for token in tokenized_text if not token.isdigit()]\n",
    "\n",
    "# Remove tokens that are one character long\n",
    "tokenized_text = [token for token in tokenized_text if len(token) > 1]\n",
    "\n",
    "# Define common stopwords/conjunctions/pronouns\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "\n",
    "# Remove common stopwords\n",
    "tokenized_text = [token for token in tokenized_text if token.lower() not in stopwords_list]\n",
    "\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605da44-ba7b-48a6-bcde-f6ea8f6af864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
